
<dom-module id="using-intel-realsense-sdk">

<template>

  <style>
    :root {
      --google-codelab-header-background: var(--secondary-color);
      --paper-blue-500: var(--secondary-color);
      --google-codelab-fab-background: var(--secondary-color);
    };

    .toc-item.completed {
      color: var(--secondary-color);
    }

    google-codelab-survey {
      border: 1px solid #19B6EE;
      background: rgba(25, 182, 238, 0.1);
      border-radius: 2px;
    }

    google-codelab-step {
      --google-codelab-step-code: {
        font-family: 'Ubuntu Mono',Helvetica,Arial;
      };
      --google-codelab-step-warn: {
        border: 1px solid #ED3146;
        background: rgba(237, 49, 70, 0.1);
      };
      --google-codelab-step-tip: {
        border: 1px solid #ECA918;
        background: rgba(236, 169, 24, 0.1);
      };
    };
  </style>


  <google-codelab id="codelabobj"
                  title="Using Intel Realsense SDK in Desktop"
                  environment="web"
                  feedback-link="https://github.com/ubuntu/codelabs/issues">
    <app-route
      route="{{route}}"
      tail="{{routeTail}}"></app-route>
    
      <google-codelab-step label="Overview" duration="1">
        <p>We are going to illustrate how to build some <a href="http://www.intel.fr/content/www/fr/fr/architecture-and-technology/realsense-overview.html" target="_blank">Intel Realsense</a> SDK samples on an ubuntu core image using the classic snap. You will be able to hook up on your <a href="https://software.intel.com/en-us/iot/hardware/joule/dev-kit" target="_blank">Intel Joule</a> and build the samples, explore some of the code and try them!</p>
<h2 class="checklist">What you&#39;ll learn</h2>
<ul class="checklist">
<li>What compose the Intel Realsense SDK.</li>
<li>How to use the classic snap as a traditional ubuntu environment for developing.</li>
<li>How to setup the SDK and build existing samples.</li>
</ul>
<h2>What you&#39;ll need</h2>
<ul>
<li>An Intel Joule device, with Ubuntu Core 16 (or classic LTS) installed on it.</li>
<li>A realsense camera (ZR300 was tested).</li>
<li>Some very basic knowledge of command line use.</li>
</ul>
<google-codelab-survey survey-id="using-intel-realsense-sdk-1">
<h4>How will you use this tutorial?</h4>
<paper-radio-group>
<paper-radio-button>Only read through it</paper-radio-button>
<paper-radio-button>Read it and complete the exercises</paper-radio-button>
</paper-radio-group>
<h4>What is your current level of experience working with snap?</h4>
<paper-radio-group>
<paper-radio-button>Novice</paper-radio-button>
<paper-radio-button>Intermediate</paper-radio-button>
<paper-radio-button>Proficient</paper-radio-button>
</paper-radio-group>
</google-codelab-survey>


      </google-codelab-step>
    
      <google-codelab-step label="Getting started" duration="2">
        <p>If you are on Ubuntu 16.04 LTS or Ubuntu 16.10, getting all the required tools is very easy. </p>
<aside class="special"><p>Note: other GNU/Linux distributions are currently in the process of building packages for snapcraft.</p>
</aside>
<h2>Downloading the SDK</h2>
<p>You will be provided with a link to a tar.bz2 file containing the SDK and all samples, we are going to detail its content here, but let&#39;s download and extract the content first!</p>
<p>Now simply run (inside the classic snap):</p>
<pre>$ sudo apt update
$ sudo apt install curl
$ curl -o realsense_sdk_beta3.tar.bz2 &lt;download_link&gt;</pre>
<p>This will download the SDK and you will get a progress bar confirming it so. Once done, let&#39;s extract its content:</p>
<pre>$ tar xf realsense_sdk_beta3.tar.bz2</pre>
<p>Now that we have the SDK extracted, let&#39;s look in detail to its content and build it!</p>


      </google-codelab-step>
    
      <google-codelab-step label="Building the SDK" duration="30">
        <h2>Starting building</h2>
<p>As building the whole SDK and dependencies is quite a long process, let&#39;s start building it right away before looking at its content.</p>
<pre>$ cd realsense_sdk_beta3
$ sudo ./install_script.sh</pre>
<p>This process will take a very long time, it will as well ensure ensure that your Intel Realsense camera is recognized. Unplug it and plug it back again once the script has finished.</p>
<h2>Realsense SDK Content</h2>
<p>While the build is proceeding, let&#39;s look at the files extracted from the bz2 archive:</p>
<pre>~/realsense_sdk_beta3$ ls
README.md
beignet-1.2.1-source.tar.gz
build
install_script.sh
librealsense_persontracking_20161220.tar.gz
librealsense_slam_20161221.tar.bz2
librealsense_v1.11.2.tar.gz
opencv_3.1.0.tar.gz
realsense_object_recognition20161208.tar.bz2
realsense_samples-v0.6.2.tar.gz
realsense_sdk_07_12_2016WW50.tar.gz
release_notes
zr300_firmware_update_prq_2.zip</pre>
<p>In addition to the README.md, install script, release note, ZR300 firmware and build directory, you see various projects embedded. Let&#39;s detail them in build dependency order:</p>
<h3>beignet1.2.1-source.tar.gz</h3>
<p>Beignet is an open source implementation of the OpenCL specification - a generic compute oriented API. This code base contains the code to run OpenCL programs on Intel GPUs which basically defines and implements the OpenCL host functions required to initialize the device, create the command queues, the kernels and the programs and run them on the GPU. The code base also contains the compiler part of the stack which is included in backend/.</p>
<h3>opencv_3.1.0.tar.gz</h3>
<p>OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. It has C++, C, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications. Written in optimized C/C++, the library can take advantage of multi-core processing. Enabled with OpenCL, it can take advantage of the hardware acceleration of the underlying heterogeneous compute platform</p>
<h3>librealsense_v1.11.2.tar.gz</h3>
<p>This project is a cross-platform library (Linux, Windows, Mac) for capturing data from the Intel® RealSense™ F200, SR300, R200, LR200 and the ZR300 cameras. This effort was initiated to better support researchers, creative coders, and app developers in domains such as robotics, virtual reality, and the internet of things. Several often-requested features of RealSense™ devices are implemented in this project, including multi-camera capture.</p>
<p>The following features are available in this release:</p>
<ul>
<li>Native streams: depth, color, infrared and fisheye.</li>
<li>Synthetic streams: rectified images, depth aligned to color and vice versa, etc.</li>
<li>Intrinsic/extrinsic calibration information.</li>
<li>Majority of hardware-specific functionality for individual camera generations (UVC XU controls).</li>
<li>Multi-camera capture across heterogeneous camera architectures (e.g. mix R200 and F200 in same application)</li>
<li>Motion-tracking sensors acquisition (ZR300 only)</li>
</ul>
<p>This project contains a certain number of samples, where you can get raw data from the camera. Those are built from the general build script. We&#39;ll detail some of them in the next step.</p>
<h3>librealsense_persontracking_20161220.tar.gz</h3>
<p>Intel® RealSense™ Person Library provides the ability to sense and recognize people, understand what they do, and interact with them. <br></p>
<p>The following features are available in this release:</p>
<ul>
<li> Person Detection:  Detects people in a scene. Provides their location through a bounding rectangle. </li>
<li>Person Tracking: Follows people within a space. Provides the person&#39;s Center of Mass (COM) and handles occlusions and cases in which the person exits the field of view (FOV).</li>
<li>Person Recognition: Identifies a person by ID. Supports databases of up to 30 registered users.</li>
<li>Body Tracking: Tracks body part movements. Provides six points of interest on the body (head, chest, shoulders, and extremities of the hands).<br>Body tracking is supported up to a distance of 2.8m from the camera.</li>
<li>Body Gestures: Understands pre-defined body movements. Supports a pointing gesture (including the pointing vector) and a wave gesture.</li>
<li>Face Tracking: Tracks face movements. Provides the face direction in values of yaw, pitch, and roll.<br></li>
</ul>
<h3>realsense_object_recognition20161208.tar.bz2</h3>
<p>Intel® RealSense™ Object Library enables machines to understand what they are viewing and provides meaning to computer vision made possible by Intel RealSense cameras. This ability facilitates more dynamic human machine interaction. Intel® RealSense™ Object Library uses a CNN-based architecture that utilizes depth to efficiently and accurately classify and localize objects. This middleware includes features for recognizing, localizing, and tracking objects from a pre-defined library. <br><br>The following features are available in this release:</p>
<ul>
<li>Object Recognition (OR): Identifies a single object in the scene based on pre-trained classifiers in a pre-defined ROI.</li>
<li>Object Localization (OD): Identifies and locates multiple objects in a scene.</li>
<li>3D Object Position: Provides the x,y,z world coordinates of the center of the object. The more exact the bounding box, the more exact this value will be. This is only available in instances where depth is available and will not be provided as a direct output of localization.</li>
<li>Object Tracking: Enables the camera to keep track of objects that have been previously localized. </li>
</ul>
<h3>librealsense_slam_20161221.tar.bz2</h3>
<p>Intel® RealSense™ SLAM Library provides Simultaneous Localization and Mapping capabilities. <br></p>
<p>The following features are available in this release: </p>
<ul>
<li>Real-time 6 degrees of freedom (6DoF) camera tracking.</li>
<li>Learning an area by associating its appearance with 6DoF coordinates, so enabling re-localization.</li>
<li>Real-time construction of a 2D occupancy map of a captured environment.</li>
</ul>
<p>SLAM is tailored for, although not limited to, robots:<br>Autonomous robot navigation requires the ability to build a high precision map of the surrounding environment and enable the robot to locate itself in the map for path planning and routing purposes.<br><br>SLAM uses the visual-inertial sensor of the RealSense camera to estimate odometry and concurrently builds a map. The constructed map can be used by applications to detect obstacles, label locations, and plan paths.</p>
<h3>realsense_sdk_07_12_2016WW50.tar.gz</h3>
<p>The Intel® RealSense™ SDK for Linux provides libraries, tools, and samples to develop applications using Intel® RealSense™ cameras, over the Intel librealsense API. <br><br>The SDK provides functionality of record and playback of camera streams for test and validation. <br><br>The SDK includes libraries which support the camera stream projection of streams into a common world-space viewpoint, and libraries which enable the use of multiple middleware modules simultaneously for common multi-modal scenarios.  </p>
<p>It features:</p>
<ol type="1" start="1">
<li>Record and Play:</li>
</ol>
<ol type="1" start="1">
<li>Record: The record module provides a utility to create a file, which can be used by the playback module to create a video source.<br>    The record module provides the same camera API as defined by the SDK (librealsense) and the record API to configure recording parameters such as output file and state (pause and resume).<br>    The record module loads librealsense to access the camera device and execute the set requests and reads, while writing the configuration and changes to the file.</li>
<li>Playback: The playback module provides a utility to create a video source from a file. <br>    The playback module provides the same camera API as defined by the SDK (librealsense), and the playback API to configure recording parameters such as input file, playback mode, seek, and playback state (pause and resume).<br>    The playback module supports files that were recorded using the Linux SDK recorder and the Windows RSSDK recorder (up to version 2016 R2).</li>
</ol>
<ol type="1" start="2">
<li>Frame data container:<br>    The SDK provides an image container for raw image access and basic image processing services, such as format conversion, mirror, rotation, and more. It caches the processing output to optimize multiple requests of the same operation.<br>    The image container includes image metadata, which may be used by any pipeline component to attach additional data or computer vision (CV) module processing output to be used by other pipeline components. The SDK uses a correlated samples container to provide access to camera images and motion sensor samples from the relevant streams, which are time-synchronized. The correlated samples container includes all relevant raw buffers, metadata, and information required to access the attached images. </li>
<li>Spatial correlation and projection:<br>    The Spatial Correlation and Projection library provides utilities for spatial mapping:</li>
</ol>
<ol type="1" start="1">
<li>Map between color or depth image pixel coordinates and real world coordinates</li>
<li>Correlate depth and color images and align them in space</li>
</ol>
<ol type="1" start="4">
<li>Pipeline:<br>    The pipeline is a class, which abstracts the details of how the cognitive data is produced by the computer vision modules.<br>    The application can focus on consuming the computer vision output, leaving the camera configuration and streaming details for the pipeline to handle.</li>
<li>Samples:</li>
</ol>
<ol type="1" start="1">
<li>Projection: The sample demonstrates how to use the different spatial correlation and projection functions, from live camera and recorded file</li>
<li>Record and Playback: The sample demonstrates how to record and play back a file while the application is streaming, with and without an active CV module,      with minimal changes to the application, compared to live streaming.</li>
<li>Video module, asynchronized: The sample demonstrates an application usage of a Computer Vision module, which implements asynchronous sample processing. </li>
<li>Video module, synchronized: The sample demonstrates an application usage of a Computer Vision module, which implements synchronous samples processing.</li>
<li>Fatal error recovery: The sample demonstrates how the application can recover from a fatal error in one of the SDK components (CV module or core module), without having to terminate.</li>
<li>Tools:</li>
</ol>
<ol type="1" start="1">
<li>Capture tool: Provides a GUI to view camera streams, create a new file from a live camera, and play a file in the supported formats. The tool provides options to render the camera or file images.</li>
<li>Projection tool: Provides simple visualization of the projection functions output, to allow human eye detection of major offsets in the projection computation.</li>
<li>System Info tool: Presents system data such as Linux name, Linux kernel version, CPU information, and more.</li>
</ol>
<ol type="1" start="7">
<li>Utilities:</li>
</ol>
<ol type="1" start="1">
<li>Log: The SDK provides a logging library, which can be used by the SDK components and the application to log meaningful events. </li>
<li>Time Sync utility: Provides methods to synchronize multiple streams of images and motion samples based on the samples time-stamp or sample number. </li>
<li>SDK Data Path utility: The SDK provides a utility to locate SDK files in the system.<br>     The utility is used by Computer Vision modules, which need to locate data files in the system that are constant for all applications (not application- or algorithm-instance specific).</li>
<li>FPS counter:  Measures the actual FPS in a specific period. You can use this utility to check the actual FPS in all software stack layers in your applications, to analyze FPS latency in those layers.<br></li>
</ol>
<h3>realsense_samples-v0.6.2.tar.gz</h3>
<p>We are going to detail the available samples in the next section!</p>


      </google-codelab-step>
    
      <google-codelab-step label="Available samples" duration="30">
        <h2>Preliminary notes</h2>
<p>Ubuntu core doesn&#39;t have any graphical server by default. Consequently, you can&#39;t run most of the samples rendering some graphical UI. However, there are a lot of samples with a _web appendix. They are spawning a web interface that you can access to the network, showing you real time data and capture from the camera.</p>
<p>Those are the examples you should focus on right now.</p>
<h2>Running your first sample</h2>
<p>Here is an example with rs_pt_tutorial_1_web:</p>
<pre>(classic)foo@localhost:~$ rs_pt_tutorial_1_web 

enabling: stream:DEPTH, 320x240x30, format:Z16

enabling: stream:COLOR, 320x240x30, format:RGB8
Device open failed, aborting...
Device open failed, aborting...
cl_get_gt_device(): error, unknown device: ffffffff

web folder path: /usr/share/librealsense/samples/rs_pt_tutorial_1_web_browser

Server: waiting for client to connect...

Best host: 192.168.0.29 of ifa type: wlan0
 &gt;&gt;&gt; point your browser to: http://192.168.0.29:8000/view.html</pre>
<p>Then, point your browser to <a href="http://192.168.0.29:8000/view.html" target="_blank">http://192.168.0.29:8000/view.html</a> (of course, your IP will be different) and you will get something like:</p>
<p><img style="max-width: 624.00px" src="img/ceac1fe2c15e7a59.png"></p>
<p>The corresponding code is in <code>~/realsense_sdk_beta3/build/realsense_samples-0.6.2/samples/pt_tutorial_1_web/cpp/main.cpp</code></p>
<p>You can spend some times here to study the various SDK examples in the same samples/ directory and execute them one after another.</p>
<p>All corresponding commands will be prepended with rs_. librealsense samples are prepended with cpp_ and their code is in <code>realsense_sdk_beta3/build/librealsense-1.11.2/examples/</code>.</p>
<h2>Realsense sample description</h2>
<p>To guide you through the samples, here is some description of the existing SDK samples:</p>
<ul>
<li>rs_or_tutorial_1: This console app demonstrates the use of librealsense, Object Library, and the Linux SDK Framework, along with the RealSense camera&#39;s depth and color sensors to identify objects in the scene. <br>Each object identified is printed on the command line along with the confidence value for the object. Objects must take up ~50% of the screen to be recognized.</li>
<li>rs_or_tutorial_2: This console app builds on the rs_or_tutorial_1 app and demonstrates how to utilize the object localization and 3D localization functionality of the Object Library module. All items with &gt;= 90% confidence will be identified, along with their bounding rectangle coordinates in the console..</li>
<li>rs_or_tutorial_3: This console app builds on the rs_or_tutorial_2 app and demonstrates how to add the Object Tracking functionality of the Object Library module, with localization and 3D localization. Objects for tracking can be as small as 1% of the screen.</li>
<li>rs_or_tutorial_1_web: This GUI app builds on the rs_or_tutorial_1 app and displays the live color preview from the camera within a browser and shows a table with a label for the object along with its confidence value.</li>
<li>rs_or_tutorial_2_web: This GUI app builds on the rs_or_tutorial_2 app and displays the live color preview from the camera within a browser and draws rectangles on the color images in the region where objects are recognized. This GUI also includes a table that shows a label for the object along with its confidence value and the 3D (x,y,z) coordinates of the object.</li>
<li>rs_or_tutorial_3_web: This GUI app builds on the rs_or_tutorial_3 app and displays the live color preview from the camera within a browser and draws rectangles on the color images in the region where objects are recognized and keeps track of the objects. This GUI also includes a table that shows a label for the object along with its confidence value and the 2D (x,y) coordinates of the object.</li>
<li>rs_or_tutorial_1_gui: This GUI/console app builds on the rs_or_tutorial_1, and demonstrates the use of librealsense, object recognition in Object Library, and the Linux SDK Framework along with the RealSense camera&#39;s depth and color sensors to identify objects in the scene. Each object identified will be listed on the command line and on a pop-up window along with the confidence value for the object.</li>
<li>rs_or_tutorial_2_gui: This GUI/console app builds on the rs_or_tutorial_2 app, and demonstrates how to utilize object localization and 3D localization. A GUI is displayed in a pop-up window along with output in the console. Recognized objects will be highlighted with a bounding box, label, and confidence level drawn in a live video stream. Other information is displayed for each recognized object in a table in the pop-up window and in the console.</li>
<li>rs_or_tutorial_3_gui: This GUI/console app builds on the rs_or_tutorial_3 app, and demonstrates how to add object tracking functionality with localization and 3D localization. Tracked objects can be as small as 1% of the screen. The coordinates of the tracked object(s) are displayed in a pop-up window and the console. Tracked objects are shown by a labeled bounding rectangle drawn over a live video stream. The user can trigger the localization function using the mouse.</li>
<li>rs_or_tutorial_4_gui: This GUI/console app builds on the rs_or_tutorial_3 app, and demonstrates how to add object tracking functionality. The coordinates of the tracked object(s) are displayed in pop-up window and the console. Tracked objects are shown the pop-up window by a labeled bounding rectangle drawn over a live video stream. The user will choose the tracked object using the mouse.</li>
<li>rs_pt_tutorial_1: This sample app demonstrates the use of libRealsense, Person Library, and the Linux SDK Framework to use the ZR300 camera&#39;s depth and color sensors to detect people in the scene. The number of people detected in the scene will be displayed as a quantity in the console.</li>
<li>rs_pt_tutorial_2: This sample app demonstrates how to analyze someone&#39;s posture. When a person is in the FOV, the app should display the following information once every second: his head direction (yaw, pitch, roll values) and his body direction (front/side/back values).</li>
<li>rs_pt_tutorial_3: This sample app demonstrates how to get the body tracking points and detect a pointing gesture. The app will display 6 body points, a &#34;Pointing Detected&#34; alert when the gesture is performed, and the pointing vector.</li>
<li>rs_pt_tutorial_1_web: This GUI app builds on the rs_pt_tutorial_1 app and displays the live color preview from the camera within a browser and draw rectangles around the person(s) detected in the camera frame and draws a color dot indicating the center of mass for the detected person in the image. There is also a table as part of the GUI that shows the person id, center of mass world coordinates (x,y,z) and the cumulative count of persons detected.</li>
<li>rs_pt_tutorial_2_web: This GUI app builds on the rs_pt_tutorial_2 app and displays the live color preview from the camera within a browser and draws rectangle around the head of a person detected in the camera frame. There is also a table as part of the GUI that shows the person id, head pose information, and the person orientation (front, left, right, etc.) with confidence score.</li>
<li>rs_pt_tutorial_3_web: This GUI app builds on the rs_pt_tutorial_3 app and displays the live color preview from the camera within a browser and draws rectangle around the person detected in the camera frame. Also draws an arrow to indicate the direction of the pointing gesture. There is also a table as part of the GUI that shows the person id, gesture origin (x,y,z) and gesture direction.</li>
<li>rs_or_pt_tutorial_1: This console app demonstrates the use of libRealsense, Object Library, and Person Library, using the ZR300 camera identify objects and persons. The name of the object, along with the confidence value will be printed in the console. Person information like person id and the 2D box coordinates will be printed in the console.</li>
<li>rs_or_pt_tutorial_1_web: This GUI app builds on the rs_or_pt_tutorial_1 app and displays the live color preview from the camera within a browser and draw rectangles around the person(s) and object(s) detected in the camera frame. There is also a table as part of the GUI that shows the person id, center of mass world coordinates(x,y,z) along with object label, object center world coordinates and confidence score.</li>
<li>rs_slam_or_pt_tutorial_1: This console app demonstrates the use of libRealsense, SLAM, Object Library, and Person Library, using the ZR300 camera to print out the current camera module position and identified objects and identified persons. The name of the object along with the confidence value will be printed in the console for identified objects. Person information like person id and the 2D box coordinates will be printed in the console.</li>
<li>rs_slam_or_pt_tutorial_1_web: This GUI app builds on the rs_slam_or_pt_tutorial_1 app and displays live fisheye and color preview, occupancy map, input and tracking fps for fisheye, depth, gyro and accelerometer frames, within a browser. Draws rectangles around recognized objects and persons in the color preview.</li>
<li>rs_slam_tutorial_1: This app demonstrates the use of librealsense and SLAM Library. The librealsense library streams depth, fish eye, and IMU data from the ZR300 camera which are used as input to the SLAM Library library. The SLAM Library outputs the camera pose (position and orientation) and occupancy map. The occupancy map is a 2D map of the surroundings that shows which areas are occupied by obstacles to navigation. It also illustrates how to save a ROS compatible occupancy PNG file and metadata .yaml file.</li>
<li>rs_slam_tutorial_1_web: This GUI app builds on the rs_slam_tutorial_1 app and displays live fisheye preview, occupancy map, input and tracking fps for fisheye, depth, gyro and accelerometer frames, within a browser.<br></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="That&#39;s all folks!" duration="1">
        <p>Congratulations! You have played with the Intel Realsense SDK, installed and enabled it on your Ubuntu Core image in the Joule. You now have as well some samples and looked at some code on using the SDK.</p>
<p>You should by now be familiar with the various parts composing the SDK, what each of them is responsible for and some basic ideas on how to use them. You also know what  features the Intel Realsense supports and have some samples on how to utilize them.</p>
<h2>Next steps</h2>
<ul>
<li>Learn some more advanced techniques on how to use your snap system looking for our others codelabs!</li>
<li>Join the snapcraft.io community by subcribing to <a href="https://lists.snapcraft.io/mailman/listinfo/snapcraft" target="_blank">our mailing list</a>.</li>
</ul>
<h2>Further readings</h2>
<ul>
<li>The <a href="http://www.intel.fr/content/www/fr/fr/architecture-and-technology/realsense-overview.html" target="_blank">Intel Realsense</a> website is a good one to learn more about this technology.</li>
<li>Check how you can <a href="http://snapcraft.io/community/" target="_blank">contact us and the broader community</a>.</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

</template>

<script>

  Polymer({

    is: 'using-intel-realsense-sdk',

    properties: {
      route: {
        type: String
      },
      backURL: {
        type: String,
        value: "/",
      },
    },

    observers: [
      '_getParams(routeTail.__queryParams)'
    ],

    ready: function() {
      
      
      this.$.codelabobj._goToHome = this._goToHome.bind(this)
    },

    _goToHome: function() {
        
        window.location.href = this.backURL;
    },

    _getParams: function(params) {
      if (!params) {
        return;
      }
      if (params.backURL !== undefined) {
          this.backURL = decodeURIComponent(params.backURL)
      }
    },

  });

</script>

</dom-module>
